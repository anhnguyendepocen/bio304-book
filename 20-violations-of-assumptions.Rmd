# Violations of Assumptions

Both t-tests and ANOVA make assumptions about the variable of interest. Namely, both assume:

- The variable of interest approximately normally distributed
- The variance of the variable is approximately the same in the groups being compared

When our data violate the assumptions of standard statistical tests, we have several options:

- Ignore violations of the assumptions, especially with large sample sizes and modest violations
- Transform the data (e.g., log transformation)
- Nonparametric methods
- Permutation tests

**Libraries**

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
library(cowplot)
```


## Graphical methods for detecting deviations from normality

As we have seen, histogram and density plot can highlight distributions that deviate from normality.

Look for:
  * distinctly non-symmetric spread -- long tails, data up against a lower or upper limit
  * multiple modes
  * Varying bin widths (histograms) and the degree of smoothing (density plots) can both be useful.


#### Example data: Marine reserves{-}

Example 13.1 in your text book describes a study aimed at understanding whether marine reserves are effective at preserving marine life. The data provided are "biomass ratios" between reserves and similar unprotected sites nearby. A ratio of one means the reserve and the nearby unprotected site had equal biomass; values greater than one indicate greater biomass in the rserve while values less than one indicate greater biomass in the unprotected site.


```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
library(cowplot)

marine.reserves <- read_csv("https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/ABD-marine-reserves.csv")

ggplot(marine.reserves, aes(x = biomassRatio, y = ..density..)) +
  geom_histogram(bins=8, color='gray', alpha=0.5) + 
  stat_density(geom="line",
               adjust = 1.25,   # adjust is in multiples of smoothing bandwidth
               color = 'firebrick', size=1.5) +
  labs(x = "Biomass ratio", y = "Probability density")
```

Both the histogram and the density plot suggest the biomass ratio distribution is strongly right skewed.

### Normal probability plots {-}

Normal probability plots are another visualization tools we saw previously for judging deviations from normality. If the observed data are approximately normally distributed, the scatter of points should fall roughly along a straight line with slope equal to the standard deviation of the data and intercept equal to the mean of the data.

```{r, fig.asp = 1}
ggplot(marine.reserves, aes(sample = biomassRatio)) + 
  geom_qq() + 
  geom_qq_line(color = 'firebrick', size = 1.5, alpha = 0.5)
```


## Formal test of normality

There are number of formal tests of normality, with the "Shapiro-Wilk" test being among the most common. The Shapiro-Wilk test is essentially a formalization of the procedure we do by eye when judging a QQ-plot.

The null hypothesis for the Shapiro-Wilk test is that the data are drawn from a normal distribution.  The test statistic for the Shapiro-Wilks tests is called $W$, and it measures the ratio of sumed deviations from the mean for a theoretical normal relative to the observed data. Small values of W are evidence of departure from normality. As in all other cases we've examined, the observed $W$ is compared to the expected sampling distribution of $W$ under the null hypothesis to estimate a P-value. Small P-values reject the null hypothesis of normality.

```{r}
shapiro.test(marine.reserves$biomassRatio)
```

## When to ignore violations of assumptions

Estimation and testing of means tends to be robust to violation of assumptions of normality and homogeneity of variance. This robustness is a consequence of the Central Limit Theorem.

However, if "two groups are being compared and both differ from normality in different ways, then even subtle deviations from normality can cause errors in the analysis (even with fairly large sample sizes)" (W&S Ch 13).


## Data transformations

A common approach to dealing with non-normal variables is to apply a mathematical function that transforms the data to more nearly normal distribution.  Analyses and inferences are then done on the transformed data.  

**Log transformation**: This widely-used transformation can only be applied to positive numbers, since `log(x)` is undefined when `x` is negative or zero. However, addition of a small positive constant can be used to ensure that all data are positive. For example, if a vector `x` includes negative numbers, then `x + abs(min(x)) + 1` results in positive numbers that can be log transformed.

**Arcsine square root transformation**: $p' = arcsin[\sqrt{p}]$ is often used when the data are proportions. If the original data are percentages, first divide by 100 to convert to proportions.

**Square root transformation**: $Y' = \sqrt{Y + K}$, where $K$ is a constant to ensure that no data points are negative. Results from the square root transformation are often very similar to log transformation.

**Square transformation**: $Y' = Y^2$. This may be helpful when the data are skewed left. .

**Antilog transformation**: $Y' = e^Y$ may be helpful if the square transformation does not resolve the problem. Only usable when all data points have the same sign.  If all data points are less than zero, take the absolute value before applying the antilog transformation.

**Reciprocal transformation**: $Y' = {\frac{1}{Y}}$ When data are right skewed, this transformation may be helpful. If all data points are less than zero, take the absolute value before applying the reciprocal transformation.

### Example: log transformation to deal with skew {-}

The biomass ratio variable in the marine reserves data is strongly right skewed. Let's see how well a log transformation does in terms of normalizing this data:

```{r, fig.width = 9, fig.height = 4}
ln.biomass.hist <- 
  ggplot(marine.reserves, aes(x = log(biomassRatio), y = ..density..)) + 
  geom_histogram(bins=8, color='gray', alpha=0.5) + 
  stat_density(geom="line",
               adjust = 1.25,   # adjust is in multiples of smoothing bandwidth
               color = 'firebrick', size=1.5) +
  labs(x = "ln[Biomass ratio]", y = "Probability density")

ln.biomass.qq <-
  ggplot(marine.reserves, aes(sample = log(biomassRatio))) + 
  geom_qq() + 
  geom_qq_line(color = 'firebrick', size = 1.5, alpha = 0.5) +
  labs(x = "ln[Biomass ratio]", y = "Theoretical quantiles")

plot_grid(ln.biomass.hist, ln.biomass.qq)
```

The log-transformed biomass ratio data is still a bit right skewed though not nearly as much as before.  Applying the Shapiro-Wilk test to this transformed data, we fail to reject the null hypothesis of normality at $\alpha = 0.05$.

```{r}
shapiro.test(log(marine.reserves$biomassRatio))
```


### Confidence intervals and data transformations {-}

Sometimes we need to transform our data before analysis, and then we want to know the confidence intervals on the original scale. In such cases, we back-transform the upper and lower CIs to the original scale of measurement.

Let's calculate 95% CIs for the log-transformed biomass ratio data:

```{r}
log.biomass.CI <- 
  marine.reserves %>%
  mutate(log.biomassRatio = log(biomassRatio)) %>%
  summarize(mean = mean(log.biomassRatio),
            sd = sd(log.biomassRatio),
            n = n(),
            SE = sd/sqrt(n),
            CI.low = mean - abs(qt(0.025, df = n-1)) * SE,
            CI.high = mean + abs(qt(0.025, df = n-1)) * SE) %>%
  select(CI.low, CI.high)

log.biomass.CI 
```

This analysis finds the 95% CI around the mean of the log transformed data ranges is `r c(log.biomass.CI$CI.low, log.biomass.CI$CI.high)`.

To understand the biological implications of this analysis, we back-transform the log data results to the original scale, computing the *antilog* by using function `exp`. After this, the back-transformed CI is now on the scale of $e^{\log X} = X$. 

Letting $\mu'$ = `mean(log.X)`, we have $0.347 < \mu' < 0.611$ on the log scale.

On the original scale:  

```{r}
log.biomass.CI %>%
  # back-transform CI to original scale
  mutate(CI.low.original  = exp(CI.low),
         CI.high.original = exp(CI.high))
```

Hence, 1.41 < geometric mean < 1.84.

The *geometric mean* equals $\sqrt[n]{x_1 x_2 \cdots x_n}$, computed by multiplying all these _n_ numbers together, then taking the $n^{th}$ root.

Biologically, "this 95% confidence interval indicates that marine reserves have 1.41 to 1.84 times more biomass on average than the control sites do" (W&S 13.3)

***

### Example: Log transformation of data with outliers

Data distributions in biology often show outliers for various traits. Examples might look like the following. In the histogram you can see a small group of outliers on the right:

```{r, echo = FALSE}
set.seed(1)
df <- data_frame(norm.dat=rnorm(200), 
                 other = rnorm(200, mean=3), 
                 outlier = runif(200))
df <- mutate(df, has_outliers = ifelse(outlier >= 0.95, other, norm.dat),
             trait = has_outliers + abs(min(has_outliers)) + 1,
             log.trait = log(trait))
df <- select(df, trait, log.trait)
```

```{r, out.width = "60%"}
head(df)

ggplot(df) + geom_histogram(aes(x = trait), bins = 50)
```

And the deviation from linearity is clear in the QQ-plot (upper right).

```{r, out.width = "60%"}
ggqqplot(df$trait)
```

We will use the Shapiro-Wilk test to check normality. With this test, the null hypothesis proposes that the data follow a normal distribution. Therefore, small P-values reject the null hypothesis of normality.

```{r}
shapiro.test(df$trait)
```

Log transformation may improve the data distribution, so that it is closer to normality:

```{r, out.width = "60%"}
ggplot(df) + geom_histogram(aes(x = log.trait), bins = 50)
ggqqplot(df$log.trait)
```

With this log transformation, the histogram and QQ-plot look reasonably normal. And the Shapiro-Wilk test on the transformed data is compatible with normality:

```{r}
shapiro.test(df$log.trait)
```

The P-value > 0.05, so we do not reject the null hypothesis of normality.

***

### Example: Zero-inflated data

Several Duke Biology labs transplant genotypes to the field and measure the fitness of each individual plant. Fitness equals the number of seeds produced, which is zero when individuals die early in life.  Such "zero-inflated" distributions cannot easily be transformed to normality, and thus require other statistical approaches for analysis.

```{r, out.width = "60%", echo=FALSE}
set.seed(1)
df <- data_frame(norm.dat = rnorm(500, mean = 1, sd = 1))
df2 <- mutate(df, 
              zero.infl = ifelse(norm.dat < 0, 0, norm.dat),
              fitness = zero.infl / mean(zero.infl))
ggplot(df2) + geom_histogram(aes(x = fitness), bins = 50) +
  labs(title="Zero-inflated distribution of fitness", 
       x = "Individual fitness",
       y = "Frequency")
```

***

### Nonparametric tests

Nonparametric tests are helpful for non-normal data distributions, especially with outliers. Typically, these tests use the ranks of the data points rather than the actual values of the data. Because they are not using all available data, nonparametric tests have lower statistical power than parametric tests. As with parametric tests, nonparametric approaches assume that the data are a random sample from the population.

**Sign test** <font color='blue'>"compares the median of a sample to a constant specified in the null hypothesis. It makes no assumption about the distribution of the measurment in the population."</font> (WS 13). However, the sign test has very low statistical power. WS example 13.4 shows an application of the sign test.

**Wilcoxon signed-rank test** This test assumes that the distribution of the data is symmetrical around the median. Given this restrictive assumption, we do not consider this further.

**Mann-Whitney U-test** compares the data distribution of two groups, without requiring the normality assumptions of the two-sample t-test. The null hypothesis of this test is that the within-group data *distributions* are the same. Therefore, it is possible to reject the null hypothesis because the distributions differ, *even if the means are the same*. The Mann-Whitney U-test is sensitive to unequal variances or different patterns of skew. Therefore, this test should be used to compare distributions, not to compare means. 

Confusingly, the Mann-Whitney U-test is also called the Mann–Whitney–Wilcoxon test, which is similar to the Wilcoxon rank-sum test. **Remember the following:**:

<font color='red'>
* Do not confuse the Wilcoxon rank-sum test with the Wilcoxon signed-rank test. They are different; we won't use either one.</font>

<font color='red'>
* We will focus on the Mann-Whitney U-test, which we will perform with function `wilcox.test`.</font>

### Mann-Whitney U example

WS 13.5 examines a rather disgusting example of sexual cannablism in sagebrush crickets. Without going into the gory details, our null hypothesis is:

* $H_0$: Time to mating is the same for female crickets that were starved or fed.
* $H_A$: Time to mating differs between these groups.

The following data were obtained in that study:

```{r}
starved <- c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)
fed <- c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)
time <- c(starved, fed)
grp <- c(rep("starved", length(starved)), rep("fed", length(fed)))
df <- data_frame(Time = time, Group = grp)
head(df)
tail(df)

ggplot(df) +
  geom_density(aes(x = Time, color = Group)) +
  labs(x = "Time to mating", y = "Density")
```

Clearly these distributions differ from each other, and are far from normal. To perform the Mann-Whitney U text, we use function `wilcox.test` ([details here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html)). This can take a data frame as input:

```{r}
wilcox.test(time ~ Group, alternative = "two.sided", paired = FALSE, data = df)
```

***

### Type I and Type II error rates

Violation of statistical assumptions (such as normality) may cause parametric tests to have an elevated probability of rejecting the null hypothesis when it is true -- an elevated Type I error rate. 

A nonparametric test may solve this problem, at the cost of failing to reject the null hypothesis when it is false (Type II errors). Thus, nonparametric tests typically have lower statistical power.

Therefore, we prefer to use parametric tests whenever possible.

***

### Permutation tests = Randomization tests

<font color='blue'>A _permutation test_ generates a null distribution for the association between two variables by repeatedly and randomly rearranging the values of one of the two variables in the data.</font> WS 13.8

```{r, echo=FALSE}
# Generate real data set and three permuted columns
set.seed(79) # set seed for random numbers
grp <- c("A", "A", "A", "A", "B", "B", "B", "B", "mean.A", "mean.B", "mean.diff")
obs.data <- c(1.1, 0.9, 1.3, 1.0, 2.1, 2.2, 1.9, 2.1, NA, NA, NA)
obs.data[9] <- mean(obs.data[1:4])
obs.data[10] <- mean(obs.data[5:8])
obs.data[11] <- obs.data[10] - obs.data[9]
df.obs <- data_frame(Group = grp, obs = obs.data)

perm.vec <- function(my.vec){
  my.vec[1:8] = sample(my.vec[1:8])
  my.vec[9]  <- mean(my.vec[1:4])
  my.vec[10] <- mean(my.vec[5:8])
  my.vec[11] <- my.vec[10] - my.vec[9]
  return(my.vec)
}
  
df.perm <- mutate(df.obs,
                  perm1 = perm.vec(obs),
                  perm2 = perm.vec(obs),
                  perm3 = perm.vec(obs))
```

To illustrate permutation of an observed data set, we create a data frame with 4 observations from group A (rows 1-4), and four observations from group B (rows 5 - 8). Then, in the next three rows, we compute the mean of group A, the mean of group B, and the difference between the means. This is shown in the two columns of the data set:

```{r}
df.obs
```

We hypothesize that:

$H_0$: The means of the two groups are equal. The difference between means equals zero.

$H_A$: The means of the two groups are not equal

`mean.diff` is our _test statistic_, and the observed value of `mean.diff` = `r df.obs$obs[[11]]`.

Next, we permute ("randomize" or "shuffle") the first eight rows. After randomization, $H_0$ is true, because we have removed any relationship between Group and the permuted data. Columns `perm1`, `perm2`, `perm3` show three reshuffled outcomes under the null hypothesis, and the last line shows the differences in group means under $H_0$.

```{r}
df.perm
```

And if we do this 1000 times, we estimate the _null distribution_ of `mean.diff` when $H_0$ is true. Using this, we can compare the observed value of our test statistic to the null distribution, in order to determine whether the observed `mean.diff` is an extreme outlier, or else represents a common occurrence under the null hypothesis. By placing the observed test statistic within the null distribution (figure below), we can estimate the P.value for our observed data.

***

Next we perform a permutation test to compare the group means for the cricket example. (Code is in the rmd file, but not shown in the html output.) We again use

$H_0$: The means of the two groups are equal. The difference between means equals zero.

```{r, echo=FALSE}
# compute basic information for the cricket data set
starved <- c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)
fed <- c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)
obs.mean.diff <- mean(starved) - mean(fed) # mean difference
len.starv <- length(starved) # length of vector
len.fed <- length(fed)       # length of vector
time <- c(starved, fed)      # concatenate two vectors
len.tot <- length(time)      # total lenght of concatenated vector
start.fed <- len.starv + 1   # beginning of second group in vector
```

```{r, echo=FALSE}
# Function to reshuffle the data and return difference in the means
mixup <- function(time, len.tot, len.starv, start.fed){
  z <- sample(time, len.tot) # permute data into random order within vector
  # Compute means, and difference between means
  mean1 <- mean(z[1:len.starv])
  mean2 <- mean(z[start.fed:len.tot])
  mean.diff <- mean1 - mean2
  return(mean.diff) # return mean diff between the (reshuffled) groups
}
```

```{r, echo=FALSE}
# Set up book-keeping for this analysis
NumPermute <-  10000 # Number of permutations
# Vector to hold differences between permuted groups
res <- vector(mode = "numeric", length = NumPermute)

# For every permutation, save mean.diff between permuted groups
set.seed(79) # set seed for random numbers
for (i in 1:NumPermute){
  res[i] <- mixup(time, len.tot, len.starv, start.fed)
}

# For observed mean difference, where is it in permuted distribution?
percentile <- ecdf(res) # get CDF within null distribution
p.value <- 2.0 * percentile(obs.mean.diff) # two tailed test

# graph permuted distribution and the observed mean diff (red line)
df.res <- data.frame(res = res)
ggplot(df.res) + 
  geom_histogram(aes(x = res), color="lightgray", fill="lightgray", bins = 60) +
  labs(title="Permuted null distribution of difference between group means", 
       x = "Difference between group means", y = "Frequency") +
  geom_vline(xintercept = obs.mean.diff, linetype = "dashed", 
             color = "red", size=0.75) + theme_bw()
```

The figure shows the permuted (null) distribution of the difference between group means, and the vertical red line shows the observed value of this test statistic. The P-value for the observed difference between means = P = `r p.value`. Therefore, we do not reject the null hypothesis. 

*Assumptions of permutation tests*: Data must represent a random sample of the population. The distribution of the variable must have similar shape in each population. (With large sample sizes, permutation tests will be robust to violations of this assumption.)

*Using permutation tests*: This approach allows flexible statistical testing for a wide variety of tests and data distributions. For example, a genetic analysis of the zero-inflated distribution of fitness (zero-inflated histogram, above) would pose no difficulty for a permutation test. This emphasizes the importance of basic coding skills, such as you have already achieved, for flexible problem solving in scientific analyses.

***

### Steps for a permutation test

<font color='blue'>Outline from WS 13.8:

* 1) "Create a permuted set of data in which the values of the response variables are randomly reordered ...
* 2) Calculate the measure of association for the permuted sample ...
* 3) Repeat the permutation process many times" ...</font>
* 4) Locate the observed test statistic within the permuted null distribution, and compute the P-value.

***

### Acknowledgements

Figures from WS13.

***
