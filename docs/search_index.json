[
["analysis-of-variance.html", "Chapter 20 Analysis of Variance 20.1 Hypotheses for ANOVA 20.2 ANOVA, assumptions 20.3 ANOVA, key idea 20.4 Partioning of sum of squares 20.5 Mathematical partitioning of sums of squares 20.6 ANOVA table 20.7 ANOVA test statistic, \\(F\\) 20.8 The F-distribution 20.9 Example, circadian rythm data 20.10 The aov() function 20.11 Visualizing the data 20.12 Total sum of squares 20.13 Group sum of squares and mean square 20.14 Error sum of squares and mean square 20.15 Combined visualization 20.16 Calculating the F-statistic 20.17 The F-distribution 20.18 The aov function 20.19 Which pairs of group means are different?", " Chapter 20 Analysis of Variance \\(t\\)-tests are the standard approach for comparing means between two groups. When you want to compare means between more than two groups a technique called “Analysis of Variance” (ANOVA) is used. 20.1 Hypotheses for ANOVA When using ANOVA to compare means, the null and alternative hypotheses are: \\(H_0\\): The means of all the groups are equal \\(H_A\\): At least one of the means is different from the others 20.2 ANOVA, assumptions ANOVA assumes: The measurements in every group represent a random sample from the corresponding population The varaible of interest is normally distributed The variance is approximately the same in all the groups 20.3 ANOVA, key idea The key idea behind ANOVA is that: If the observations in each group are drawn from populations with equal means (and variances) then the variation between group means should be similar to the inter-individual variation within groups. 20.4 Partioning of sum of squares Another way to think about ANOVA is as a “partitioning of variance”. The total variance among all the individuals across groups can be decomposed into: variance of the group means around the “grand mean”; variance of individuals around the group means. However, rather than using variance we use sums of square deviations around the respectives means (usually shortened to “sums of squares”). This decomposition is represented visually in the figure below: Figure 20.1: Whitock &amp; Schluter, Fig 15.1.2 – Illustrating the partitioning of sum of squares into \\(MS_{group}\\) and \\(MS_{error}\\) components. 20.5 Mathematical partitioning of sums of squares Variable \\(X\\) with a total sample of \\(N\\) observations, partitioned ito \\(k\\) groups. The sample size of the g-th group is \\(n_g\\), and thus \\(N = \\sum_{g=1}^{k} n_g\\). Let \\(\\overline{X}\\) indicate the grand mean of \\(X\\) and \\(\\overline{X}_g\\) indicate the mean of \\(X\\) in the g-th group. Total sums of squares We call the sum of the squared deviations around the grand mean the “total sum of sqaures” (\\(SS_\\text{total}\\)). \\[ SS_\\text{total} = \\sum_{i=1}^N (x_i-\\overline{X})^2 \\] The total degrees of freedom is: \\(df_\\text{total} = N - 1\\) Group sum of squares and group mean square deviation The sum of squared deviations of the group means around the grand mean is called the “group sum of squares”: \\[ SS_\\text{group} = \\sum_{g=1}^kn_g(\\overline{X}_g - \\overline{X})^2 \\] The degrees of freedom associated with the group sum of squares is: \\(df_\\text{group} = k - 1\\) We define the “group mean squared deviation” as: \\[ MS_\\text{group} = \\frac{SS_\\text{group}}{k-1} \\] Error sum of squares and error mean square deviation The sum of squared deviations of the individual observations about their respective group means is called the “error sum of squares”: \\[ SS_\\text{error} = \\sum_{g=1}^k\\sum_{i=1}^{n_g} (x_{i,g} - \\overline{X}_g)^2 \\] The degrees of freedom associated with the error sum of squares is: \\(df_\\text{error} = N - k\\) We define the “error mean squared deviation” as: \\[ MS_\\text{error} = \\frac{SS_\\text{error}}{N-k} \\] 20.6 ANOVA table The results of an analysis of variance test are often presented in the form of a table organized as follows: Source \\(SS\\) \\(df\\) \\(MS\\) \\(F\\) Group \\(SS_\\text{group}\\) \\(k-1\\) \\(MS_\\text{group}\\) \\(MS_\\text{group}/MS_\\text{error}\\) Error \\(SS_\\text{error}\\) \\(N-k\\) \\(MS_\\text{error}\\) Total \\(SS_\\text{total}\\) \\(N-1\\) \\(MS_\\text{total}\\) 20.7 ANOVA test statistic, \\(F\\) The test statistic used in ANOVA is designated \\(F\\), and is based on the ratio of the group mean square deviation (\\(MS_\\text{groups}\\)) to the error mean square deviation&quot; (\\(MS_\\text{error}\\)): \\[ F = \\frac{\\text{group mean square}}{\\text{error mean square}} = \\frac{\\text{MS}_\\text{group}}{\\text{MS}_\\text{error}} \\] Under the null hypothesis, the between group and within group variances are similar and thus the \\(F\\) statistic should be approximately 1. Large values of the \\(F\\)-statistic means that the between group variance exceeds the within group variance, indicating that at least one of the means is different from the others 20.8 The F-distribution The sampling distribution of the \\(F\\)-statistic is called the \\(F\\)-distribution. The \\(F\\)-distribution depends on two parameters: 1) the degrees of freedom associated with the group sum of squares, $df_\\text{group} = k - 1$; 2) the degrees of freedom associated with the error sum of squares, $df_\\text{error} = N - k$; We designate a particular \\(F\\)-distribution as \\(F_{k-1,N-k}\\). 20.9 Example, circadian rythm data Your textbook describes an examplar data set from a study designed to test the effects of light treatment on circadian rhythms (see Whitlock &amp; Schluter, Example 15.1). The investigators randomly assigned 22 individuals to one of three treatment groups and measured phase shifts in melatonin production. The treatment groups were: control group (8 indiviuals) light applied on the back of the knee (7 individuals) light applied to the eyes (7 individuals) These data are available at: ABD-circadian-rythms.csv Libraries library(tidyverse) library(magrittr) library(cowplot) library(broom) set.seed(20180113) Load the data circadian &lt;- read_csv(&quot;https://raw.githubusercontent.com/bio304-class/bio304-course-notes/master/datasets/ABD-circadian-rythms.csv&quot;) 20.10 The aov() function As you would suspect, there is a built in R function to carry out ANOVA. This function is designated aov(). aov takes a formula style argument where the variable of interest is on the left, and the grouping variable indicated on the right. 20.11 Visualizing the data As we usually do, let’s start by visualizing the data. We’ll create a point plot depicting the observations colored by treatment group. # we&#39;re going to re-use our jitterying across plots so # assign it to a variable pd &lt;- position_jitter(width=0.2, height=0) point.plot &lt;- circadian %&gt;% ggplot(aes(x=treatment, y=shift, color=treatment, group=row.names(circadian))) + geom_point(position = pd) + ylim(-3,1)+ labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;) grand.mean &lt;- mean(circadian$shift) total.plot &lt;- circadian %&gt;% ggplot(aes(x=treatment, y=shift, color=treatment, group=row.names(circadian))) + geom_linerange(aes(ymin = grand.mean, ymax = shift), position = pd) + geom_hline(yintercept = grand.mean, linetype=&#39;dashed&#39;) + ylim(-3,1)+ labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of observations around the grand mean (dashed line)&quot;) + theme(plot.title = element_text(size=9)) plot_grid(point.plot, total.plot) 20.12 Total sum of squares We call the sum of the squared deviations around the grand mean the “total sum of sqaures” (\\(SS_\\text{total}\\)). Here’s how we calculate it and the associated degrees of freedom: # total sum of squares total.table &lt;- circadian %&gt;% summarize(sum.squares = sum((shift - grand.mean)**2), df = n() - 1) total.table ## # A tibble: 1 x 2 ## sum.squares df ## &lt;dbl&gt; &lt;dbl&gt; ## 1 16.6 21 20.13 Group sum of squares and mean square Next we turn to variation of the group means around the grand mean. We use group_by and summarize to calculates group means and the group deviates (the difference between the group means and the grand mean): group.df &lt;- circadian %&gt;% group_by(treatment) %&gt;% summarize(n = n(), group.mean = mean(shift), grand.mean = grand.mean, group.deviates = group.mean - grand.mean) Let’s visualize the difference of the group means from the grand mean: group.plot &lt;- group.df %&gt;% ggplot(aes(x = treatment, y = group.mean, color=treatment)) + geom_linerange(aes(ymin = grand.mean, ymax = group.mean), size=2) + geom_point(size = 3, alpha = 0.25) + geom_hline(yintercept = grand.mean, linetype=&#39;dashed&#39;) + ylim(-3,1) + labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of group means around the grand mean&quot;) + theme(plot.title = element_text(size=9)) group.plot Now we calculate the group sum of squares (\\(SS_\\text{group}\\)) and the group mean square (\\(MS_\\text{group}\\)). This calculation takes into account the size of each group (for the group sum of squares) and the degrees of freedom associated with the number of groups (for the group mean square). group.table &lt;- group.df %&gt;% summarize(SS = sum(n * group.deviates**2), k = n(), df = k-1, MS = SS/df) 20.14 Error sum of squares and mean square Next we turn to variation of the individual observations around the group means, which is the basis of the error sum of squares and mean square. error.df &lt;- circadian %&gt;% group_by(treatment) %&gt;% mutate(group.mean = mean(shift), error.deviates = shift - group.mean) %&gt;% summarize(SS = sum(error.deviates**2), n = n()) We can visualize these individual deviates around the group means as so: error.plot &lt;- circadian %&gt;% group_by(treatment) %&gt;% mutate(group.mean = mean(shift)) %&gt;% ggplot(aes(x = treatment, y = shift, color = treatment)) + geom_point(aes(y = group.mean),size=3,alpha=0.1) + geom_linerange(aes(ymin = group.mean, ymax = shift), position = pd) + ylim(-3,1) + labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of observations around the groups means&quot;) + theme(plot.title = element_text(size=9)) error.plot Now we calculate the error sum of squares (\\(SS_\\text{error}\\)) and the error mean square (\\(MS_\\text{error}\\)). Here the degrees of freedom is the total sample size minus the number of groups. error.table &lt;- error.df %&gt;% summarize(SS = sum(SS), k = n(), N = sum(n), df = N - k, MS = SS/df) 20.15 Combined visualization We can combine our three plots created above into a single figure using cowplot::plot_grid: combined.plot &lt;-plot_grid(total.plot, group.plot, error.plot, labels = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), nrow = 1) combined.plot 20.16 Calculating the F-statistic Having calculated our estimates of between group variance and within group variance (\\(MS_\\text{group}\\) and $MS_) we’re now ready to calculate the \\(F\\) test statistic. F.stat &lt;- group.table$MS/error.table$MS F.stat ## [1] 7.289449 20.17 The F-distribution Our calculated F statistic is much larger than 1. To calculate the probability of observing an F-statistic this large or greater under the null hypothesis, we can need to examine the F-distribution. The F-distribution has two degree of freedom parameters, indicating the degrees of freedom associated with the group variance and the degrees of freedom associated with the error variance. Here is an illustration of the F-distribution, \\(F_{2,19}\\) f &lt;- seq(0, 10, length.out = 200) df1 &lt;- 2 df2 &lt;- 19 f.density &lt;- df(f, df1, df2) ggplot(data_frame(F = f, Density = f.density), aes(x = F, y = Density)) + geom_line() + labs(title = &quot;F-distribution with df1=2, df2 = 19&quot;) We can use the F distribution function, pf() to lookup the probability of getting a value of 7.2894487 or larger under the null hypothesis: # degrees of freedom for group and error sum of squares df.group &lt;- group.table$df df.error &lt;- error.table$df # use the F distribution function pf(F.stat, df.group, df.error, lower.tail = FALSE) ## [1] 0.004472271 20.17.1 Critical values of the F-distribution If we wanted to know what the critical F value is for a corresponding type I error rate we can use the qf() function: # the critical value of F for alpha = 0.05 qf(0.05, df.group, df.error, lower.tail = FALSE) ## [1] 3.521893 20.18 The aov function As you would suspect, there is a built in R function to carry out ANOVA. This function is designated aov(). aov takes a formula style argument where the variable of interest is on the left, and the grouping variable indicated on the right. anova.circadian &lt;- aov(shift ~ treatment, data = circadian) The summary function applied to the aov fit will print out a typical ANOVA table and calculate the associated P-value for the \\(F\\) test statistic: summary(anova.circadian) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 7.224 3.612 7.289 0.00447 ** ## Residuals 19 9.415 0.496 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you want the ANOVA table in a form you can compute with, the broom::tidy function we explored previously comes in handy: tidy(anova.circadian) ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 treatment 2 7.22 3.61 7.29 0.00447 ## 2 Residuals 19 9.42 0.496 NA NA 20.19 Which pairs of group means are different? If an ANOVA indicates that at least one of the group means is different than the others, the next question is usually “which pairs are different?”. There are slighly different tests for what are called “planned” versus “unplanned” comparisons. Your textbook discusses the differences between these two types Here we focus on a common test for unplanned comparisons, called the Tukey Honest Significant Differences test (referred to as the Tukey-Kramer test in your textbook). The Tukey HSD test controls for the “family-wise error rate”, meaning it tries to keep the overall false positive (Type I error) rate at a specified value. 20.19.1 Tukey-Kramer test The function TukeyHSD implements the Tukey-Kramer test. The input to TukeyHSD is the fit from aov: TukeyHSD(anova.circadian) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = shift ~ treatment, data = circadian) ## ## $treatment ## diff lwr upr p adj ## eyes-control -1.24267857 -2.1682364 -0.3171207 0.0078656 ## knee-control -0.02696429 -0.9525222 0.8985936 0.9969851 ## knee-eyes 1.21571429 0.2598022 2.1716263 0.0116776 Here again, the broom::tidy function comes in handy: tidy(TukeyHSD(anova.circadian)) ## # A tibble: 3 x 6 ## term comparison estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 treatment eyes-control -1.24 -2.17 -0.317 0.00787 ## 2 treatment knee-control -0.0270 -0.953 0.899 0.997 ## 3 treatment knee-eyes 1.22 0.260 2.17 0.0117 The Tukey HSD test by default give us 95% confidence intervals for the differences in means between each pair of groups, and an associated P-value for the null hypothesis of equal means between pairs. Interpretting the results above, we see that we fail to reject the null hypothesis of equal means for the knee and control treatment groups (i.e. we have no statistical support to conclude they are different). However, we reject the null hypothesis for equality of means between control and eye treatments and between knee and eye treatements. We have evidence that light treatments applied to the eye cause a mean negative shift in the phase of melatonin production relative to control and knee treatment groups. "]
]
